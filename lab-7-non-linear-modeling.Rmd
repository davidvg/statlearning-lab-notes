---
title: "Lab 7 - Non-Linear Modeling"
subtitle: "An Introduction to Statistical Learning"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will be using the `Wage` dataset
```{r}
pacman::p_load(ISLR, splines)
attach(Wage)
```

# 1. Polynomial Regression

## Fitting the model

Our goal is to produce two plots, one showing `wage` vs `age` and other showing `wage>250` vs `age`.

We start by fitting a linear model with powers of `age`:
```{r}
poly.fit = lm(wage ~ poly(age, 4), data = Wage)
summary(poly.fit)
```

## Making predictions

We create a grid for `age` in which we will make predictions:
```{r}
agelims = range(age)
age.grid = seq(from=agelims[1], to=agelims[2])
```
Now we make the predictions and compute standard errors:
```{r}
poly.pred = predict(poly.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
We create a matrix containing the standard error intervals:
```{r}
se.bands = cbind(poly.pred$fit - 2*poly.pred$se.fit,
                 poly.pred$fit + 2*poly.pred$se.fit)
```
We now produce the first plot, showing `wage` vs `age` with a confidence interval of $95\%$:
```{r, fig.dim=c(4, 4),  fig.align='center'}
plot(age, wage, xlim = agelims, cex=.5, col='darkgrey')
lines(age.grid, poly.pred$fit, lwd = 1, col = 'blue')
matlines(age.grid, se.bands, lwd = 1, col = 'blue', lty = 2)
```
Creating the second plot requires a little more work. We start by selecting the degree for the polynomial on `age`. To do that we will use `anova()`:
```{r}
fit.1 = lm(wage ~ age, data = Wage)
fit.2 = lm(wage ~ poly(age, 2), data = Wage)
fit.3 = lm(wage ~ poly(age, 3), data = Wage)
fit.4 = lm(wage ~ poly(age, 4), data = Wage)
fit.5 = lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
The output of the ANOVA analysis shows that the p-value comparing `fit.1` and `fit.2` is essentially 0, which means that considering both models are equivalent ($H_0$), the probability of obtaining the performance difference between them that ANOVA found is almost 0, so `fit.2` is better than `fit.1`. The same happens for `fit.3` and arguibly for `fit.4`.

We will use a 4-degree polynomial to fit the data to a *qualitative* model with `glm()`, in which the target is the probability of `wage>250`, so we need `family=binomial`:
```{r}
poly.4.fit = glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
```
Now we make predictions for the age grid:
```{r}
poly.4.pred = predict(poly.4.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
The default prediction type is `link`, which for a default `binomial` model are *log-odds*, probabilities on *logit* scale:

$$
log \left ( \frac {p(Y=1 \mid X)} {1 - p(Y=1 \mid X)}  \right ) = X \beta
$$
Using `type="response"`, which gives the actual predicted probabilities, seems the correct choice here, but the confidence intervals we obtained this way would have negative values.

We need to convert the *logit* probabilities to actual probabilities:

$$
p(Y=1 \mid X) = \frac {\exp{(X \beta)}} {1 + \exp{(X \beta)}}
$$
```{r}
pfit = exp(poly.4.pred$fit) / (1 + exp(poly.4.pred$fit))
```
And for the confidence interval:
```{r}
se.bands.logit = cbind(poly.4.pred$fit - 2 * poly.4.pred$se.fit,
                       poly.4.pred$fit + 2 * poly.4.pred$se.fit)
se.bands = exp(se.bands.logit) / (1 + exp(se.bands.logit))
```
We can now create the second plot:
```{r, fig.dim=c(6, 4),  fig.align='center'}
plot(age, I(wage > 250), xlim = agelims, type='n', ylim = c(0, .2))
points(jitter(age), I(wage > 250)/5, cex = .5, pch = '|', col = 'darkgrey')
lines(age.grid, pfit, lwd = 1, col = 'blue')
matlines(age.grid, se.bands, lwd = 1, lty = 2, col = 'blue')
grid()
```

# 2. Step Functions

The `cut()` function divides the range of the data into intervals and assigns each data point to one of those intervals, returning an ordered *categorical* variable:
```{r}
cut(age, 4)[1:5]
```
Labels can be passed to `cut()` to name the different levels or intervals.

To get the total count of observations for each interval `table()` is used:
```{r}
table(cut(age, 4))
```
We can fit a linear model using these levels that creates *dummy* variables:
```{r}
step.fit = lm(wage ~ cut(age, 4), data=Wage)
coef(summary(step.fit))
```


# 3. Splines

## B-Splines

The `bs()` method, included in the `splines` library, creates an entire matrix of basis functions for splines with the given set of knots. In this case we will specify fixed knots for 3 values of `age`: 25, 40 and 60:
```{r}
bs(age, knots=c(25, 40, 60))[1:6,]
```

### Fitting the model

A linear model is fitted using these basis expansions as predictors:
```{r}
spline.fit = lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
coef(summary(spline.fit))
```
Degrees of freedom can be specified instead of knots, using `df`. This generates a spline with wknots at uniform quantiles of the data.

### Making predictions

```{r}
spline.pred = predict(spline.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
Let's plot the results:
```{r, fig.dim=c(6, 4),  fig.align='center'}
lwd = 1.0
plot(age, wage, col='darkgray', cex=.5)
lines(age.grid, spline.pred$fit, lwd=lwd, col='blue')
lines(age.grid, spline.pred$fit - 2*spline.pred$se.fit,
      lty = 'dashed', col = 'blue')
lines(age.grid, spline.pred$fit + 2*spline.pred$se.fit,
      lty = 'dashed', col = 'blue')
grid()
```

## Natural Splines

To fit a *natural spline* the `ns()` function is used:
```{r}
nat.fit = lm(wage ~ ns(age, knots = c(25, 40, 60)), data = Wage)
nat.pred = predict(nat.fit, newdata = list(age = age.grid), se.fit = TRUE)
```

## Smoothing Splines

The `smooth.spline()` method is used. The syntax is different than before.

The number of degrees of freedom can be specified using `df`, or the built-in *LOOCV* method can be used to select the best value for `df`:
```{r}
smooth.fit = smooth.spline(age, wage, cv=TRUE)
```
There's no need to make predictions when using a smoothing spline, as they are already computed in the `y` component of the fitted spline.

## Comparing the results

Let's plot all the splines together:

```{r, fig.dim=c(6, 4),  fig.align='center'}
lwd = 1.0
plot(age, wage, col='darkgray', cex=.5, ylim = c(40, 140))
# B-Spline
lines(age.grid, spline.pred$fit, lwd=lwd, col='blue')
# Natural Spline
lines(age.grid, nat.pred$fit, col='red', lwd=lwd)
# Smoothing Spline
lines(smooth.fit, col='darkgreen', lwd=lwd)

legend('bottomright',
       legend = c('B-spline', 'Natural Spline', 'Smoothing Spline'),
       col = c('blue', 'red', 'darkgreen'),
       lty=1, cex=.7)
```

# 4. Local Regression


