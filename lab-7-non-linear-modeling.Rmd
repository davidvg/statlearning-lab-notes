---
title: "Lab 7 - Non-Linear Modeling"
subtitle: "An Introduction to Statistical Learning"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will be using the `Wage` dataset
```{r}
pacman::p_load(ISLR, splines)
attach(Wage)
```

# 1. Polynomial Regression

## Fitting the model

Our goal is to produce two plots, one showing `wage` vs `age` and other showing `wage>250` vs `age`.

We start by fitting a linear model with powers of `age`:
```{r}
poly.fit = lm(wage ~ poly(age, 4), data = Wage)
summary(poly.fit)
```

## Making predictions

We create a grid for `age` in which we will make predictions:
```{r}
agelims = range(age)
age.grid = seq(from=agelims[1], to=agelims[2])
```
Now we make the predictions and compute standard errors:
```{r}
poly.pred = predict(poly.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
We create a matrix containing the standard error intervals:
```{r}
se.bands = cbind(poly.pred$fit - 2*poly.pred$se.fit,
                 poly.pred$fit + 2*poly.pred$se.fit)
```
We now produce the first plot, showing `wage` vs `age` with a confidence interval of $95\%$:
```{r, fig.dim=c(4, 4),  fig.align='center'}
plot(age, wage, xlim = agelims, cex=.5, col='darkgrey')
lines(age.grid, poly.pred$fit, lwd = 1, col = 'blue')
matlines(age.grid, se.bands, lwd = 1, col = 'blue', lty = 2)
```
Creating the second plot requires a little more work. We start by selecting the degree for the polynomial on `age`. To do that we will use `anova()`:
```{r}
fit.1 = lm(wage ~ age, data = Wage)
fit.2 = lm(wage ~ poly(age, 2), data = Wage)
fit.3 = lm(wage ~ poly(age, 3), data = Wage)
fit.4 = lm(wage ~ poly(age, 4), data = Wage)
fit.5 = lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
The output of the ANOVA analysis shows that the p-value comparing `fit.1` and `fit.2` is essentially 0, which means that considering both models are equivalent ($H_0$), the probability of obtaining the performance difference between them that ANOVA found is almost 0, so `fit.2` is better than `fit.1`. The same happens for `fit.3` and arguibly for `fit.4`.

We will use a 4-degree polynomial to fit the data to a *qualitative* model with `glm()`, in which the target is the probability of `wage>250`, so we need `family=binomial`:
```{r}
poly.4.fit = glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
```
Now we make predictions for the age grid:
```{r}
poly.4.pred = predict(poly.4.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
The default prediction type is `link`, which for a default `binomial` model are *log-odds*, probabilities on *logit* scale:

$$
log \left ( \frac {p(Y=1 \mid X)} {1 - p(Y=1 \mid X)}  \right ) = X \beta
$$
Using `type="response"`, which gives the actual predicted probabilities, seems the correct choice here, but the confidence intervals we obtained this way would have negative values.

We need to convert the *logit* probabilities to actual probabilities:

$$
p(Y=1 \mid X) = \frac {\exp{(X \beta)}} {1 + \exp{(X \beta)}}
$$
```{r}
pfit = exp(poly.4.pred$fit) / (1 + exp(poly.4.pred$fit))
```
And for the confidence interval:
```{r}
se.bands.logit = cbind(poly.4.pred$fit - 2 * poly.4.pred$se.fit,
                       poly.4.pred$fit + 2 * poly.4.pred$se.fit)
se.bands = exp(se.bands.logit) / (1 + exp(se.bands.logit))
```
We can now create the second plot:
```{r, fig.dim=c(4, 4),  fig.align='center'}
plot(age, I(wage > 250), xlim = agelims, type='n', ylim = c(0, .2))
points(jitter(age), I(wage > 250)/5, cex = .5, pch = '|', col = 'darkgrey')
lines(age.grid, pfit, lwd = 1, col = 'blue')
matlines(age.grid, se.bands, lwd = 1, lty = 2, col = 'blue')
grid()
```

# 2. Step Functions

The `cut()` function divides the range of the data into intervals and assigns each data point to one of those intervals, returning an ordered *categorical* variable:
```{r}
cut(age, 4)[1:5]
```
Labels can be passed to `cut()` to name the different levels or intervals.

To get the total count of observations for each interval `table()` is used:
```{r}
table(cut(age, 4))
```
We can fit a linear model using these levels that creates *dummy* variables:
```{r}
step.fit = lm(wage ~ cut(age, 4), data=Wage)
coef(summary(step.fit))
```


# 3. Splines

## B-Splines

The `bs()` method, included in the `splines` library, creates an entire matrix of basis functions for splines with the given set of knots. In this case we will specify fixed knots for 3 values of `age`: 25, 40 and 60:
```{r}
bs(age, knots=c(25, 40, 60))[1:6,]
```

### Fitting the model

A linear model is fitted using these basis expansions as predictors:
```{r}
spline.fit = lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
coef(summary(spline.fit))
```
Degrees of freedom can be specified instead of knots, using `df`. This generates a spline with wknots at uniform quantiles of the data.

### Making predictions

```{r}
spline.pred = predict(spline.fit, newdata = list(age = age.grid), se.fit = TRUE)
```
Let's plot the results:
```{r, fig.dim=c(4, 4),  fig.align='center'}
lwd = 1.0
plot(age, wage, col='darkgray', cex=.5)
lines(age.grid, spline.pred$fit, lwd=lwd, col='blue')
lines(age.grid, spline.pred$fit - 2*spline.pred$se.fit,
      lty = 'dashed', col = 'blue')
lines(age.grid, spline.pred$fit + 2*spline.pred$se.fit,
      lty = 'dashed', col = 'blue')
grid()
```

## Natural Splines

To fit a *natural spline* the `ns()` function is used:
```{r}
nat.fit = lm(wage ~ ns(age, knots = c(25, 40, 60)), data = Wage)
nat.pred = predict(nat.fit, newdata = list(age = age.grid), se.fit = TRUE)
```

## Smoothing Splines

The `smooth.spline()` method is used. The syntax is different than before.

The number of degrees of freedom can be specified using `df`, or the built-in *LOOCV* method can be used to select the best value for `df`:
```{r}
smooth.fit = smooth.spline(age, wage, cv=TRUE)
```
There's no need to make predictions when using a smoothing spline, as they are already computed in the `y` component of the fitted spline.

## Comparing the results

Let's plot all the splines together:

```{r, fig.dim=c(4, 4),  fig.align='center'}
lwd = 1.0
plot(age, wage, col='darkgray', cex=.5, ylim = c(40, 140))
# B-Spline
lines(age.grid, spline.pred$fit, lwd=lwd, col='blue')
# Natural Spline
lines(age.grid, nat.pred$fit, col='red', lwd=lwd)
# Smoothing Spline
lines(smooth.fit, col='darkgreen', lwd=lwd)

legend('bottomright',
       legend = c('B-spline', 'Natural Spline', 'Smoothing Spline'),
       col = c('blue', 'red', 'darkgreen'),
       lty=1, cex=.7)
```

# 4. Local Regression

The `loess()` function (included in the `stats` library) performs local regression.

## Fitting the data
Let's fit two local models, with `span` values of 0.2 and 0.5; this means that each neighborhood consists of 20% and 50% of the observations:
```{r}
local.fit1 = loess(wage ~ age, data = Wage, span = 0.2)
local.fit2 = loess(wage ~ age, data = Wage, span = 0.5)
```

## Making predictions
```{r}
local.pred1 = predict(local.fit1, newdata = data.frame(age=age.grid))
local.pred2 = predict(local.fit2, newdata = data.frame(age=age.grid))
```

## Plotting the results
```{r, fig.dim=c(6, 4),  fig.align='center'}
plot(age, wage, xlim = agelims, cex = .5, col = 'darkgrey')
lines(age.grid, local.pred1, col = 'red', lwd = 1)
lines(age.grid, local.pred2, col = 'blue', lwd = 1)
legend('topright',
       legend = c('Span = 0.2', 'Span = 0.5'),
       col = c('red', 'blue'),
       lty = 1, lwd = 1, cex = 0.8)
```

The `locfit` library can also be used for fitting local regression models.


# 4. Generalized Additive Models

GAMs are linear regression models using an appropiate choice of basis functions, so they can be fitted using `lm()`. Here a GAM is fitted using a natural spline with 4 dof for `year`, another natural spline wit h5 dof for `age` and the `education` variable as is, because it's a qualitative variable:
```{r}
gam.fit.ns = lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
```

To fit more general GAMs, using smoothing splines and other components that can't be expressed in terms of basis functions, the `gam` library is used.
```{r}
pacman::p_load(gam)
```

## Fitting the model
The `s()` function from `gam` is used to use smoothing splines, whereas `lo()` performs local regression. We create a smoothing spline for `year` with 4 dof and we use local regression for `age` with a `span` value of 0.7:
```{r}
gam.fit = gam(wage ~ s(year, df = 4) + lo(age, span=.7) + education, data = Wage)
```

## Plotting the fitted model
Results can be plotted:
```{r, fig.dim=c(8, 4),  fig.align='center'}
par(mfrow=c(1,3))
plot(gam.fit, se=TRUE, col='blue')
```

## Exploring linearity for `year`

In the plots the function of `year` shows some linearity. An ANOVA test can be used to determine which of the following models is best:

- A GAM that excludes `year`.
- A GAM that uses a linear function of `year`.
- A Gam that uses a spline function of `year` (our fitted model)

```{r}
gam.mod.1 = gam(wage ~ lo(age, span = 0.7) + education, data = Wage)
gam.mod.2 = gam(wage ~ year + lo(age, span = 0.7) + education, data = Wage)
gam.mod.3 = gam.fit
```
Let's do the ANOVA test:
```{r}
anova(gam.mod.1, gam.mod.2, gam.mod.3, test = 'F')
```
The result shows evidence that a GAM with a linear function of `year` is better than a GAM that does not include `year` at all, but doesn't show evidence that a non-linear function of `year` is needed.

Let's plot the second model:
```{r, fig.dim=c(8, 4),  fig.align='center'}
par(mfrow=c(1,3))
plot(gam.mod.2, se=TRUE, col='blue')
```

## Creating interactions with `lo()`

`lo()` can be used to create interactions before calling `gam()`. Here a model is fitted creating an interaction between `year` and `age`:
```{r}
gam.lo.inter = gam(wage ~ s(year, df = 4) + lo(year, age, span = 0.5) + education,
                   data = Wage)
```

## Fitting a Logistic Regression GAM

It can be done using `family="binomial"`. Here `I()` is used again to create a binary response variable:
```{r, fig.dim=c(8, 4),  fig.align='center'}
gam.log = gam(I(wage > 250) ~ year + s(age, df = 5) + education,
              data = Wage, family = 'binomial')

par(mfrow=c(1,3))
plot(gam.log, se = TRUE, col = 'blue')
```
In the `1. < HS Grad` category the error bar is huge, due to the fact that there are no high earners for that category:
```{r}
table(education, I(wage > 250))
```
It's best then to fit the GAM using all the `education` categories but that one:
```{r, fig.dim=c(8, 4),  fig.align='center'}
gam.log.noHS = gam(I(wage > 250) ~ year + s(age, df = 5) + education,
                   data = Wage, family = 'binomial',
                   subset = (education != "1. < HS Grad"))

par(mfrow=c(1,3))
plot(gam.log.noHS, se = TRUE, col = 'blue')
```
