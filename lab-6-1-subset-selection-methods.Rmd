---
title: "Lab 6.1 - Subset Selection Methods"
subtitle: "An Introduction to Statistical Learning"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
We will use the `Hitters` dataset.
```{r}
library(ISLR)
sum(is.na(Hitters))
```
There are $59$ missing observations for `Salary` so we need to make some cleaning:
```{r}
Hitters = na.omit(Hitters)
attach(Hitters)
```

Subset Selection is done using the `regsubsets()` method, included in the `leaps` library. RSS is used to measure which model is "best".
```{r}
library(leaps)
```


## 1. Best Subset Selection
By default `regsubsets()` reports result up to the best eight-variable model. We can change this with the parameter `nvmax`:
```{r}
regfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
reg.summary
```
An asterisk indicates that a given variable is included in the corresponding model.

If we want to force a varible to appear in the model we can use the `force.in` parameter, with a list of the column indices. In a similar way, we can force a variable to not be included in the model using `force.out`.

`summary()` also includes different statistics used to select the best model: $R^2$ (`rsq`), Residual Sum of Squares (`rss`), Adjusted $R^2$ (`adjr2`), Mallow's $C_p$ (`cp`) and Bayesian Information Criterion (`bic`). `which` is a matrix defining which variables are included in each model.
```{r}
names(reg.summary)
```
$\textrm{MSE} = \textrm{RSS}/n$ is generally an underestimate of the test MSE (the model is fitted to get the smallest  *training* error, but the same model does not have to be the one with the lowest *test* error, because the training MSE tipycally decreases when we add more variables, but the test MSE can increase). Therefore, training set RSS and training set $R^2$ are not the best metrics to select the best model. On the other hand, $C_p$, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC) and Adjusted $R^2$ are computed using techniques for *adjusting* the training error for the model size.

To decide which model to use we can plot some statistics:
```{r, fig.dim=c(10, 6),  fig.align='center'}
par(mfrow=c(2, 2), mar=c(4, 4, 3, 4))
plot(reg.summary$rss, type='b', xlab = 'Model size', ylab = 'RSS'); grid()
plot(reg.summary$adjr2, type='b', xlab = 'Model size', ylab = 'Adj R^2'); grid()
plot(reg.summary$cp, type='b', xlab = 'Model size', ylab = 'CP'); grid()
plot(reg.summary$bic, type='b', xlab = 'Model size', ylab = 'BIC'); grid()
```
`regsubsets()` also has a built-in `plot()` function to plot the selected variables for the best model with a given number of predictors, according to the specified loss metric:
```{r, fig.dim=c(10, 8),  fig.align='center'}
par(mfrow=c(2, 2), mar=c(4, 4, 3, 4))
plot(regfit.full, scale = 'r2')
plot(regfit.full, scale = 'adjr2')
plot(regfit.full, scale = 'Cp')
plot(regfit.full, scale = 'bic')
```
The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. In this case when we use `adjr2` we obtain a 12-variable model, a 11-variable model for `Cp` and a 7-variable model for `bic`.

To get the coefficients for the 7-variable model:
```{r}
coef(regfit.full, 7)
```


## 2. Forward and Backward Stepwise Selection

Forward and Backward stepwise selection are done with `regsubsets()` using the parameter `method`.
```{r}
regfit.fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = 'forward')
regfit.bwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = 'backward')
```
The method returns a similar output than before, with asterisks indicating when a variable has been included in a model.

Like before, `summary(fitted_model)` has information for the different statistics.


## 3. Choosing among models using the Validation Set approach and Cross-Validation

### The Validation Set approach
We create training and test subsets:
```{r}
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test = (!train)
```
Now we perform model selection using the training set:
```{r}
regfit.best = regsubsets(Salary~., data = Hitters[train,], nvmax = 19)
```
Computing the validation set error is more complicated than with other methods, as there is no `predict()` function for `regsubsets()`. We first make a model matrix from the test data. `model.matrix()` creates a model matrix by expanding factors (`League`, `Division`, `NewLeague`) to a set of dummy variables and expanding interactions similarly:
```{r}
test.mat = model.matrix(Salary~., data = Hitters[test,])
head(test.mat)
```
Now we get the coefficients for the best model obtained by `regsubsets()` for each model size `i`, and multiply them into the appropiate columns of the test model matrix to form the predictions and compute the test MSE:
```{r}
val.errors = rep(NA, 19)
for (i in 1:19) {
  # Get the coefficients for the i-th model
  coef.i = coef(regfit.best, id = i)
  # Make a new model matrix containing only the variables for the i-th model
  test.mat.i = test.mat[, names(coef.i)]
  # Multiply the new matrix by the coefficients for the i-th model
  pred = test.mat.i %*% coef.i
  # Compute validation set errors
  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}
```
We can make a function to do this prediction process:
```{r}
pred.regsubsets = function(obj, newdata, id, ...) {
  form = as.formula(obj$call[[2]])
  mat = model.matrix(form, newdata)
  coef.i = coef(obj, id=id)
  xvars = names(coef.i)
  mat[, xvars] %*% coef.i
}
```
Plotting the validation errors we see that the model with the smallest error is the one with 7 variables:
```{r fig.dim=c(6, 4), fig.align='center'}
plot(val.errors, type='b', xlab = 'Model size', ylab = 'Validation error'); grid()
```
The minimum validation error occurs for the 7-variable model:
```{r}
ix = which.min(val.errors)
cat(sprintf("Model Size: %d [MSE = %.2f]", ix, val.errors[ix]))
```
Now we can use the complete dataset to create the final 7-variable model:
```{r}
regfit.final = regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.final, id = 7)
```
Different selection methods can select different variables for the same model size.

To see how well it predicted:
```{r}
preds.val = pred.regsubsets(regfit.final, Hitters[, -19], id = 7)
cat(sprintf("Model Size: %d [MSE = %.2f]", ix, mean((preds.val - Hitters$Salary)^2)))
```

### K-Fold Cross-Validation

We perform best subset selection *within each of the k training sets*. First we assign each row in *Hitters* to one of the $k$ folds:
```{r}
set.seed(1)
k = 10
folds = sample(1:k, nrow(Hitters), replace = T)
head(folds, 20)
```
Create an error matrix:
```{r} 
cv.errors = matrix(data = 0, nrow = k, ncol = 19,
                   dimnames = list(paste(1:k), paste(1:19)))
```
We now perform cross-validation in a loop:
```{r}
for (fold in 1:k) {
  best.fit = regsubsets(Salary~.,
                        data = Hitters[folds!=fold,],
                        nvmax = 19)
  for (nvar in 1:19) {
    pred = pred.regsubsets(obj = best.fit,
                           newdata = Hitters[folds==fold,],
                           id = nvar)
    err = Hitters$Salary[folds==fold]
    cv.errors[fold, nvar] = as.double(mean((err - pred)^2))
  }
}
```
We compute the mean error for each model size, by computing the mean for every fold given a model size (mean of the columns):
```{r}
mean.cv.errors = apply(cv.errors, 2, mean)
```
And we plot the results:
```{r fig.dim=c(6, 4), fig.align='center'}
plot(mean.cv.errors, type='b', xlab = 'Model size', ylab = 'Mean CV error')
grid()
```
In this case the best model is one with 10 variables:
```{r}
coef(best.fit, id = 10)
```
And the error is:
```{r}
ix = which.min(mean.cv.errors)
preds.val = pred.regsubsets(best.fit, Hitters[, -19], id = ix)
cat(sprintf("Model Size: %d [MSE = %.2f]", ix, mean((preds.val - Hitters$Salary)^2)))
```

